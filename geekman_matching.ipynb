{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7120a530",
   "metadata": {},
   "source": [
    "#### file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71ce94b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# dictionary file path\n",
    "dictionary_filepath = 'resources/curated_word_list.txt'\n",
    "\n",
    "# slangification map \n",
    "# slangCharMap()\n",
    "slangification_map_filepath = 'resources/slangification_map.txt'\n",
    "\n",
    "# input file containing username pair to match\n",
    "input_filename = 'data/test.csv'\n",
    "\n",
    "# output file\n",
    "output_filename = 'output/sim_score.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa8b1c3",
   "metadata": {},
   "source": [
    "#### set minimum length of dictionary word to consider as a token (English word/Token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300c4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurable\n",
    "min_length = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974b133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3edfeb6a",
   "metadata": {},
   "source": [
    "#### read word list from dictionary\n",
    "##### all words in the dictionary are already in lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85d04e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words/phrases in the dictionary 46019\n"
     ]
    }
   ],
   "source": [
    "word_list = set();\n",
    "with open(dictionary_filepath) as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        word_list.add(line.strip())\n",
    "\n",
    "print('number of words/phrases in the dictionary', len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838291be",
   "metadata": {},
   "source": [
    "#### read slangCharMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96a386a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slanCharMap {'0': ['o', 'd'], '1': ['i', 'l', 't', 'j'], '2': ['z'], '3': ['e', 's'], '4': ['a', 'r'], '5': ['s'], '6': ['g'], '7': ['t', 'l', 'j', 'r'], '8': ['b'], '9': ['g'], '@': ['a'], '!': ['i'], '$': ['s']}\n"
     ]
    }
   ],
   "source": [
    "with open(slangification_map_filepath, 'r') as f:\n",
    "    lines = [list(line.strip()) for line in f.readlines()]\n",
    "\n",
    "slangification_map = {}\n",
    "for line in lines:\n",
    "    \n",
    "    slangification_map[line[0]] = line[1:]\n",
    "\n",
    "    \n",
    "print('slanCharMap', slangification_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c9942",
   "metadata": {},
   "source": [
    "#### create alphabet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee77e200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e'] ['A', 'B', 'C', 'D', 'E'] ['0', '1', '2', '3', '4'] ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '@', '!', '$']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "alphabet_lower = list(string.ascii_lowercase)\n",
    "alphabet_upper = list(string.ascii_uppercase)\n",
    "digits = list(string.digits)\n",
    "slang_alphabet = list(slangification_map.keys())\n",
    "\n",
    "print(alphabet_lower[:5], alphabet_upper[:5], digits[:5], slang_alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4770b1",
   "metadata": {},
   "source": [
    "#### Weighted Interval Scheduling (https://github.com/aladdinpersson/Algorithms-Collection-Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "598f729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "\n",
    "\n",
    "class WeightedIntervalScheduling(object):\n",
    "    def __init__(self, I):\n",
    "        self.I = sorted(I, key=lambda tup: tup[1])  # (key = lambda tup : tup[1])\n",
    "        self.OPT = []\n",
    "        self.solution = []\n",
    "\n",
    "    def previous_intervals(self):\n",
    "        start = [task[0] for task in self.I]\n",
    "        finish = [task[1] for task in self.I]\n",
    "        p = []\n",
    "\n",
    "        for i in range(len(self.I)):\n",
    "            # finds idx for which to input start[i] in finish times to still be sorted\n",
    "            idx = bisect.bisect(finish, start[i]) - 1\n",
    "            p.append(idx)\n",
    "\n",
    "        return p\n",
    "\n",
    "    def find_solution(self, j):\n",
    "        if j == -1:\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            if (self.I[j][2] + self.compute_opt(self.p[j])) > self.compute_opt(j - 1):\n",
    "                self.solution.append(self.I[j])\n",
    "                self.find_solution(self.p[j])\n",
    "\n",
    "            else:\n",
    "                self.find_solution(j - 1)\n",
    "\n",
    "    def compute_opt(self, j):\n",
    "        if j == -1:\n",
    "            return 0\n",
    "\n",
    "        elif (0 <= j) and (j < len(self.OPT)):\n",
    "            return self.OPT[j]\n",
    "\n",
    "        else:\n",
    "            return max(\n",
    "                self.I[j][2] + self.compute_opt(self.p[j]), self.compute_opt(j - 1)\n",
    "            )\n",
    "\n",
    "    def weighted_interval(self):\n",
    "        if len(self.I) == 0:\n",
    "            return 0, self.solution\n",
    "\n",
    "        self.p = self.previous_intervals()\n",
    "\n",
    "        for j in range(len(self.I)):\n",
    "            opt_j = self.compute_opt(j)\n",
    "            self.OPT.append(opt_j)\n",
    "\n",
    "        self.find_solution(len(self.I) - 1)\n",
    "\n",
    "        return self.OPT[-1], self.solution[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a69f83a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dc2c068",
   "metadata": {},
   "source": [
    "#### find meaningful words/name phrases from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15bd0703",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: username\n",
    "Output: \n",
    "    dict_chunk_count: count of dictionary chunks\n",
    "    coverage: sum of the length of the dictionary chunks only\n",
    "    chunks: all chunks including dictionary chunks\n",
    "\"\"\"\n",
    "def get_dict_chunks(username):\n",
    "\n",
    "    \n",
    "    n = len(username)\n",
    "    next_list = [(-1, -1, -1) for _ in range(n)]\n",
    "    \n",
    "\n",
    "    i = n-1\n",
    "    while i >= 0:\n",
    "\n",
    "        next_list[i] = (i, i + 1, 1)\n",
    "        \n",
    "\n",
    "        if i + min_length <=n and username[i] in alphabet_lower:\n",
    "            \n",
    "            local_optimal = 0\n",
    "            \n",
    "            curr_length = min_length\n",
    "            for end in range(i + min_length -1, n):\n",
    "                \n",
    "                if username[i: i+curr_length] in word_list:\n",
    "                    \n",
    "\n",
    "                    next_length = 0\n",
    "                    if i + curr_length < n:\n",
    "                        \n",
    "                        next_end = next_list[i + curr_length][1]\n",
    "                        next_length = next_end - (i+curr_length)\n",
    "                    \n",
    "                    if curr_length + next_length >= local_optimal:\n",
    "                        local_optimal = curr_length + next_length\n",
    "                        next_list[i] = (i, i + curr_length, curr_length)\n",
    "                \n",
    "                curr_length += 1\n",
    "\n",
    "        i = i-1\n",
    "\n",
    "        \n",
    "    weightedinterval = WeightedIntervalScheduling(next_list)\n",
    "    _, chunks = weightedinterval.weighted_interval()\n",
    "    \n",
    "    \n",
    "    dict_chunks = [chunk[2] for chunk in chunks if chunk[2] > 1]\n",
    "    dict_chunk_count = len(dict_chunks)\n",
    "    coverage = sum(dict_chunks)\n",
    "    \n",
    "    return (dict_chunk_count, chunks, coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4ea91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abfbf07a",
   "metadata": {},
   "source": [
    "#### generate all possible transformation using slangCharMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1789fff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def slangify(username):\n",
    "    \n",
    "    slang_chars_found = []\n",
    "    for char in slangification_map:\n",
    "        \n",
    "        if char in username:\n",
    "            slang_chars_found.append(char)\n",
    "    \n",
    "    transformed_usernames = []\n",
    "    if len(slang_chars_found):\n",
    "        \n",
    "        transformed_usernames.append(username)\n",
    "        for slang_char in slang_chars_found:\n",
    "            \n",
    "            new_transformed_usernames = []\n",
    "            replace_chars = slangification_map[slang_char]\n",
    "            for replace_char in replace_chars:\n",
    "                \n",
    "                for transformed_username in transformed_usernames:\n",
    "                    new_transformed_usernames.append(\\\n",
    "                        transformed_username.replace(slang_char, \\\n",
    "                            replace_char))\n",
    "            transformed_usernames = new_transformed_usernames\n",
    "            \n",
    "    return set(transformed_usernames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee6c1f",
   "metadata": {},
   "source": [
    "#### TokenBasedChunkification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "789e72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocess: we convert username in lowercase, discard unrelated characters\n",
    "               unrelated means outside set(alphabet_lower + slang_alphabet)\n",
    "\n",
    "Input: username\n",
    "Output: chunkification\n",
    "\n",
    "def: get_dict_chunks(username): (dict_chunk_count, chunks, coverage)\n",
    "\"\"\"\n",
    "\n",
    "def get_dict_token_based_chunkification(username):\n",
    "    \n",
    "    if username.isnumeric():\n",
    "        return []\n",
    "    \n",
    "    preprocess = lambda uname : ''.join([char for char in list(uname.lower()) if char in set(alphabet_lower + slang_alphabet)])\n",
    "    username = preprocess(username)\n",
    "    \n",
    "    chunkification_no_slang = get_dict_chunks(username)\n",
    "    token_based_chunkification = [username[chunk[0]: chunk[1]] \\\n",
    "             for chunk in chunkification_no_slang[1] ]\n",
    "    \n",
    "    slangified_usernames = slangify(username)\n",
    "    opt_chunkification_slang = chunkification_no_slang \n",
    "    opt_slangified_username = None\n",
    "    if len(slangified_usernames):\n",
    "        \n",
    "        for slangified_username in slangified_usernames:\n",
    "            \n",
    "            chunkification_slang = get_dict_chunks(slangified_username)\n",
    "            \n",
    "            optimal_cond = \\\n",
    "                (chunkification_slang[2] > opt_chunkification_slang[2]) or \\\n",
    "                (chunkification_slang[2] == opt_chunkification_slang[2] and \\\n",
    "                    chunkification_slang[0] < opt_chunkification_slang[0]) \n",
    "            \n",
    "            if  optimal_cond:\n",
    "                opt_chunkification_slang = chunkification_slang\n",
    "                opt_slangified_username = slangified_username\n",
    "        \n",
    "        if opt_slangified_username:\n",
    "            token_based_chunkification = \\\n",
    "            [opt_slangified_username[chunk[0]: chunk[1]] \\\n",
    "                 if chunk[2] > 1 else username[chunk[0]: chunk[1]] \\\n",
    "                     for chunk in opt_chunkification_slang[1] ]\n",
    "    \n",
    "    return [token for token in token_based_chunkification if len(token) >= min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ecfff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "931bfe0a",
   "metadata": {},
   "source": [
    "#### DigitBasedChunkification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fe52f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def get_digit_based_chunkification(username):\n",
    "    \n",
    "    username = username.lower()\n",
    "    letter_groups = re.findall(\"([^0-9]+)\", username)\n",
    "    digit_groups = re.findall(\"([0-9]+)\", username)\n",
    "    \n",
    "    letter_groups = [ chunk.strip() for chunk in letter_groups ]\n",
    "    digit_groups = [ chunk.strip() for chunk in digit_groups ]\n",
    "    \n",
    "    return (letter_groups, digit_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6b3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1902c4b3",
   "metadata": {},
   "source": [
    "#### SymbolBasedChunkification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c7ff4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_symbol_based_chunkification(username):\n",
    "    \n",
    "    username = username.lower()\n",
    "    chunks = re.split('[^a-zA-Z0-9]+', username) \n",
    "    \n",
    "    return [ chunk for chunk in chunks if len(chunk) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17f753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec22e0fa",
   "metadata": {},
   "source": [
    "#### CapitalizationBasedChunkification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a0a0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_capital_letter_based_chunkification(username):\n",
    "    \n",
    "    if username.islower():\n",
    "        return []\n",
    "    \n",
    "    start = 0\n",
    "    chunks = []\n",
    "    for i in range(1, len(username)):\n",
    "        \n",
    "        if username[i] in alphabet_upper:\n",
    "            chunks.append(username[start:i].lower().strip())\n",
    "            start = i\n",
    "    \n",
    "    chunks.append(username[start:len(username)].lower())\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6249d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "623edc9d",
   "metadata": {},
   "source": [
    "#### findUsernameWithoutSymbolDigit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c779a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_username_without_symbol_digit(username):\n",
    "    \n",
    "    username = username.lower()\n",
    "    chunks = re.split('[^a-z]+', username)\n",
    "    \n",
    "    return ''.join(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d1c8d",
   "metadata": {},
   "source": [
    "#### getChunkListLength (|L|)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "009b321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunkification_length(chunks):\n",
    "    \n",
    "    return sum([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96594a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81771124",
   "metadata": {},
   "source": [
    "#### function to apply for creating column of list attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51d8dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_value_for_list_attrs(list_vals):\n",
    "    \n",
    "    return ','.join(list_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31739db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f28cb1d",
   "metadata": {},
   "source": [
    "#### new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cb0c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_1, lower_2 = 'lower_1', 'lower_2'\n",
    "w_o_sym_dig_1, w_o_sym_dig_2 = 'w_o_sym_dig_1', 'w_o_sym_dig_2'\n",
    "cap_ltr_1, cap_ltr_2 = 'cap_ltr_1', 'cap_ltr_2'\n",
    "sym_1, sym_2 = 'sym_1', 'sym_2'\n",
    "dig_1, dig_2 = 'dig_1', 'dig_2'\n",
    "dict_1, dict_2 = 'dict_1', 'dict_2'\n",
    "\n",
    "sim_score = 'sim_score'\n",
    "username_1, username_2 = 'username_1', 'username_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18dfa9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98feb200",
   "metadata": {},
   "source": [
    "#### calculate similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b477dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from py_stringmatching.similarity_measure.levenshtein import Levenshtein\n",
    "from py_stringmatching.similarity_measure.monge_elkan import MongeElkan\n",
    "from py_stringmatching.similarity_measure.jaccard import Jaccard\n",
    "\n",
    "def get_similarity_score(pair_row):\n",
    "    \n",
    "    scores = []\n",
    "    monge_elkan_method = MongeElkan(sim_func=Levenshtein().get_sim_score)\n",
    "    levenshtein_method = Levenshtein()\n",
    "    \n",
    "    ########### username_lower\n",
    "    src_lower_len, target_lower_len = len(pair_row[lower_1]), len(pair_row[lower_2])\n",
    "    if src_lower_len == 0 or target_lower_len == 0:\n",
    "        score_on_username_lower = 0.0\n",
    "    \n",
    "    else:\n",
    "        score_on_username_lower = levenshtein_method.get_sim_score(pair_row[lower_1], \n",
    "                                                              pair_row[lower_2])\n",
    "    scores.append(score_on_username_lower)\n",
    "    \n",
    "    ########### without_sym_dig\n",
    "    src_feature_len, target_feature_len = len(pair_row[w_o_sym_dig_1]), len(pair_row[w_o_sym_dig_2])\n",
    "    if src_feature_len == 0 or target_feature_len == 0:\n",
    "        score_on_without_sym_dig = 0.0\n",
    "    \n",
    "    else:\n",
    "        score_on_without_sym_dig = levenshtein_method.get_sim_score(pair_row[w_o_sym_dig_1], \n",
    "                                                              pair_row[w_o_sym_dig_2])\n",
    "        \n",
    "#         weight = (src_feature_len + target_feature_len)/(src_lower_len + target_lower_len)\n",
    "#         score_on_without_sym_dig = score_on_without_sym_dig * weight\n",
    "        \n",
    "    scores.append(score_on_without_sym_dig)\n",
    "    \n",
    "    ########### cap_chunks\n",
    "    chunk_list_1, chunk_list_2 = pair_row[cap_ltr_1].split(','), pair_row[cap_ltr_2].split(',')\n",
    "    src_feature_len, target_feature_len = get_chunkification_length(chunk_list_1), \\\n",
    "                                            get_chunkification_length(chunk_list_2)\n",
    "    \n",
    "    if src_feature_len == 0 or target_feature_len == 0:\n",
    "        \n",
    "        score_on_cap_chunks = 0.0\n",
    "    else:\n",
    "        if len(chunk_list_1) < len(chunk_list_2):\n",
    "            chunk_list_1, chunk_list_2 = chunk_list_2, chunk_list_1\n",
    "        score_on_cap_chunks = Jaccard().\\\n",
    "                                    get_raw_score(chunk_list_1, chunk_list_2)\n",
    "        \n",
    "#         weight = (src_feature_len + target_feature_len)/(src_lower_len + target_lower_len)\n",
    "#         score_on_cap_chunks = score_on_cap_chunks * weight\n",
    "        \n",
    "    scores.append(score_on_cap_chunks)\n",
    "    \n",
    "    ########### sym_chunks\n",
    "    chunk_list_1, chunk_list_2 = pair_row[sym_1].split(','), pair_row[sym_2].split(',')\n",
    "    src_feature_len, target_feature_len = get_chunkification_length(chunk_list_1), \\\n",
    "                                            get_chunkification_length(chunk_list_2)\n",
    "        \n",
    "    if src_feature_len == 0 or target_feature_len == 0:\n",
    "        \n",
    "        score_on_sym_chunks = 0.0\n",
    "    else:\n",
    "        if len(chunk_list_1) < len(chunk_list_2):\n",
    "            chunk_list_1, chunk_list_2 = chunk_list_2, chunk_list_1\n",
    "        score_on_sym_chunks = monge_elkan_method.get_raw_score(chunk_list_1, chunk_list_2)\n",
    "        \n",
    "#         weight = (src_feature_len + target_feature_len)/(src_lower_len + target_lower_len)\n",
    "#         score_on_sym_chunks = score_on_sym_chunks * weight\n",
    "        \n",
    "    scores.append(score_on_sym_chunks)\n",
    "    \n",
    "    ########### dig_chunks\n",
    "    chunk_list_1, chunk_list_2 = pair_row[dig_1].split(','), pair_row[dig_2].split(',')\n",
    "    src_feature_len, target_feature_len = get_chunkification_length(chunk_list_1), \\\n",
    "                                            get_chunkification_length(chunk_list_2)\n",
    "        \n",
    "    if src_feature_len == 0 or target_feature_len == 0:\n",
    "        \n",
    "        score_on_dig_chunks = 0.0\n",
    "    else:\n",
    "        if len(chunk_list_1) < len(chunk_list_2):\n",
    "            chunk_list_1, chunk_list_2 = chunk_list_2, chunk_list_1\n",
    "        score_on_dig_chunks = monge_elkan_method.get_raw_score(chunk_list_1, chunk_list_2)\n",
    "        \n",
    "#         weight = (src_feature_len + target_feature_len)/(src_lower_len + target_lower_len)\n",
    "#         score_on_dig_chunks = score_on_dig_chunks * weight\n",
    "        \n",
    "    scores.append(score_on_dig_chunks)\n",
    "    \n",
    "    \n",
    "    ########### dict_token_chunks\n",
    "    chunk_list_1, chunk_list_2 = pair_row[dict_1].split(','), pair_row[dict_2].split(',')\n",
    "    src_feature_len, target_feature_len = get_chunkification_length(chunk_list_1), \\\n",
    "                                            get_chunkification_length(chunk_list_2)\n",
    "        \n",
    "    if src_feature_len == 0 or target_feature_len == 0:\n",
    "        \n",
    "        score_on_dict_token_chunks = 0.0\n",
    "    else:\n",
    "        if len(chunk_list_1) < len(chunk_list_2):\n",
    "            chunk_list_1, chunk_list_2 = chunk_list_2, chunk_list_1\n",
    "        score_on_dict_token_chunks = monge_elkan_method.get_raw_score(chunk_list_1, chunk_list_2)\n",
    "        \n",
    "        weight = (src_feature_len + target_feature_len)/(src_lower_len + target_lower_len)\n",
    "        score_on_dict_token_chunks = score_on_dict_token_chunks * weight\n",
    "        \n",
    "    scores.append(score_on_dict_token_chunks)\n",
    "    \n",
    "#     print(scores)\n",
    "    \n",
    "    return max(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51563caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed7fa02",
   "metadata": {},
   "source": [
    "#### creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4330107f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of usernames (48, 2)\n",
      "a few of the username pairs\n",
      "    username_1    username_2\n",
      "0  0bscurec0de  ObscureCoder\n",
      "1     0v3rride      override\n",
      "2     0verl0ad      0verload\n",
      "3    10ca1h0st     localh0st\n",
      "4      3ndG4me       EndGam3\n",
      "\n",
      "matching is successful\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(input_filename, names=[username_1, username_2])\n",
    "print('total number of usernames', df.shape)\n",
    "\n",
    "df[username_1] = df[username_1].astype(str)\n",
    "df[username_2] = df[username_2].astype(str)\n",
    "\n",
    "print('a few of the username pairs')\n",
    "print(df.head(5))\n",
    "print()\n",
    "\n",
    "#### create features\n",
    "\n",
    "# findLower\n",
    "df[lower_1] = df.apply(lambda row: row.username_1.lower(), axis = 1)\n",
    "df[lower_2] = df.apply(lambda row: row.username_2.lower(), axis = 1)\n",
    "\n",
    "# findUsernameWithoutSymbolDigit\n",
    "df[w_o_sym_dig_1] = df.apply(lambda row: get_username_without_symbol_digit(row.username_1), axis = 1)\n",
    "df[w_o_sym_dig_2] = df.apply(lambda row: get_username_without_symbol_digit(row.username_2), axis = 1)\n",
    "\n",
    "# findCapitalLetterBasedChunkification\n",
    "df[cap_ltr_1] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_capital_letter_based_chunkification(row.username_1)), axis = 1)\n",
    "df[cap_ltr_2] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_capital_letter_based_chunkification(row.username_2)), axis = 1)\n",
    "\n",
    "\n",
    "# findSymbolBasedChunkification        \n",
    "df[sym_1] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_symbol_based_chunkification(row.username_1)), axis = 1)\n",
    "df[sym_2] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_symbol_based_chunkification(row.username_2)), axis = 1)\n",
    "\n",
    "# findDigitBasedChunkification\n",
    "df[dig_1] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_digit_based_chunkification(row.username_1)[0]), axis = 1)\n",
    "df[dig_2] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_digit_based_chunkification(row.username_2)[0]), axis = 1)\n",
    "\n",
    "# findDictionaryTokenBasedChunkification        \n",
    "df[dict_1] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_dict_token_based_chunkification(row.username_1)), axis = 1)\n",
    "df[dict_2] = df.apply(lambda row: create_column_value_for_list_attrs(\\\n",
    "                                        get_dict_token_based_chunkification(row.username_2)), axis = 1)\n",
    "\n",
    "# find similarity\n",
    "df[sim_score] = df.apply(lambda row: get_similarity_score(row), axis = 1)\n",
    "df[sim_score] = df[sim_score].round(3)\n",
    "\n",
    "print(\"matching is successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80285fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5e6619e",
   "metadata": {},
   "source": [
    "#### filtering columns to write in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "91eca435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[username_1, username_2, sim_score]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80a1c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0bscurec0de', 'ObscureCoder', 0.9],\n",
       " ['0v3rride', 'override', 1.0],\n",
       " ['0verl0ad', '0verload', 1.0],\n",
       " ['10ca1h0st', 'localh0st', 1.0],\n",
       " ['3ndG4me', 'EndGam3', 1.0]]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.tolist()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "812c16f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_filename, index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e138d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c47d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e7289",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
